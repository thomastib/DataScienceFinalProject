---
title: "Final Project: Geospatial Regressions"
subtitle: "Data Science for Public Policy"
authors: "Akbar Syed Naqvi, Thomas Tiberghien Alvarez"
format: 
  html:
    code-line-numbers: true
editor: visual
execute: 
  warning: false
self-contained: true
urlcolor: blue
---

## Background and Motivation

Geographic data is everywhere. This type of data also explains many social processes and individual decisions. As this kind of data becomes more ubiquitous, it is important to understand where and how to use it, and how to extract meaningful insights from geographical properties

Regressions are considered the bread and butter of data analysis. They have been long used in economics and social science to establish causal relationships, but data science has utilized regression methods as a backbone for prediction. Spatial regressions take the power of regressions and add an element of spatial dependency.

Space can impact how one generates data and be a form of systematic error in our analysis. Spatial analysis in its modern form has been implemented and iterated upon through many subjects: biology through plant distributions and animal movement, epidemiology through disease mapping, and most notably economics through spatial econometrics, Spatial structure helps regression model in two ways

1.  Process of generating data is spatial e.g. price of homes often dependent on area

2.  Geography can allow you to assess structure of errors and mispredictions if they are systematic, often times errors are assumed to be independently and identically distributed, but this assumption falls apart with spatial data

Spatial regression is also easier to visualize than just rows and columns of data. This makes it easier to explain insights more broadly, rather than trying to tell a story through a few data points.Â 

## Methods

### Spatial Autocorrelation

The first important question to ask when doing regression is whether it is appropriate to include a spatial element. This might seem obvious in some cases, but not necessarily all the time. A series of tests have thus been developed to test for what is called spatial autocorrelation. Spatial autocorrelation is a measure of how similar the values of a variable are within a certain geographic area. If spatial autocorrelation is not accounted for, regression models might over or underestimate relationships between dependent and independent variables and undermine the validity of the models. The most common spatial autocorrelation tests are:

-   Moran's I test

-   Geary's C test

-   Getis-Ord G test

#### Moran's I test

This test computes an index based on the distance between points. An index of -1 means that the data is evenly spread out, an index of 0 means that the data is random, and an index of 1 means that the data is clumped together. This index is then compared to critical values to see if there is significant spatial autocorrelation (based on a null hypothesis that there isn't)

#### Geary's C test

This test is very similar to Moran's I test but is more sensitive to local spatial autocorrelation. In other words, whereas Moran's tests the global spatial autocorrelation, Geary's measures the pattern of spatial correlation within sub-regions of the data. The results are also the different to Moran's, with values closer to 0 representing positive spatial autocorrelation and higher than 1 representing negative spatial autocorrelation.

#### Getis-Ord G test

The Getis-Ord G test is a statistical test used to identify spatial patterns, such as hot spots and cold spots, in data. This test is based on a measure called the G statistic, which ranges from -infinity to +infinity. A positive G statistic indicates a hot spot, where the values of the dependent variable are higher than expected given their spatial location. A negative G statistic indicates a cold spot, where the values of the dependent variable are lower than expected given their spatial location. As before, the G statistic can then be compared to a critical value to determine whether the spatial patterns in the data are statistically significant.

### Spatial Weighting

Once you determine that there is spatial autocorrelation, it then becomes important to account for the effects of spatial proximity on the relationship between the dependent and independent variables. In geospatial regression, spatial weighting involves assigning a weight to each data point based on its distance from other data points. This weight is used to adjust the regression model, allowing it to take into account the fact that the values of the dependent variable are likely to be more similar to the values of other nearby data points than to data points that are farther away. There are also several ways to assign weights to data points:

-   Inverse distance weighting: Taking the inverse of the distance between a data point and all other data points. This method is based on the assumption that the values of the dependent variable are more likely to be similar to the values of nearby data points than to data points that are farther away. This metric is sensitive to the choice of the distance metric (meters vs kilometers), and produce unrealistic spatial weights if the data contains outliers or clusters.

-   K-nearest neighbor weighting: This method assigns a weight to each data point based on the number of other data points (k) that are within a certain distance of the data point. This method can produce unrealistic spatial weights if the data contains clusters of different sizes.

-   Rook contiguity weighting: This method assigns a weight based on whether it is contiguous (i.e., touching) with other data points. A data point is given a weight of 1 if it is contiguous with at least one other data point, and a weight of 0 if it is not. This method is easy to understand, but it only gives a binary measure and does not take into account the actual distance between points.

------------------------------------------------------------------------

Source: Geographic Data Science in Python

Regression models can sometimes display clustering in the error terms, which is a potential violation of i.i.d assumption in linear models. To interrogate whether or not there is clustering, one can observe distribution of error terms dis-aggregated by a spatial element of concern, visualizes how the median income in a state's counties differs by their distance to a major metro area, for example. An example of a of more sophisticated test that can be used to decipher a spatial element is a spatial autocorrelation test. Often times in non-spatial regressions, one can use spatial autocorrelation tests like Moran's I, Geary's c, or Getis and Ord's G-statistic on residuals. If spatial autocorrelations exist, this can justify the use of spatial model

    -   Often times in non-spatial regressions, one can use spatial autocorrelation tests like Moran's I, Geary's c, or Getis and Ord's G-statistic on residuals

**Spatial Weights**

Spatial questions target specific information about the spatial configuration of an outcome variable. When it comes to spatial analysis, we are often building a topography, a mathematical structure that expresses connectivity between observations. This weighting can help us create a *geographically weighted regression*; local version of spatial regression that generates parameters dis aggregated by spatial units of analysis , partial weights are one way to express this topology. Examples of spatial weights include

-   Contiguity Weights - A contiguous pair of spatial objects who share a common border,

-   Distance based weights - Proximity based, often observing the nearest neighbors, requires defining k nearest neighbors with spatial information used to calculate centroids

-   Block Weights - We weigh spatial elements based on a specific geographic group they belong to, and any elements not in the group disconnected from those observations, e.g. connecting counties to states

**Bringing Space Into a Regression**

Spatial regression involves introducing geographic space or context into our regressions when we feel that it plays a role or can act as a proxy for other unobserved variables. There are a variety of methods one can use to do this.

-   Spatial Feature Engineering

    -   In data science, feature engineering involves applying domain knowledge to raw data in order to structure it in a way that is meaningful, in other words, transforming a dataset to prepare it for analysis

    -   Geography is one of the best ways to introduce domiain knowledge into a data science problem.

    -   Spatial feature engineering is the process of developing additional information from raw data using geographical knowledge

    -   Simplest example -

------------------------------------------------------------------------

## Application

Source: https://walker-data.com/census-r/modeling-us-census-data.html

Now that we've discussed the background and various methods used in spatial regressions, let's utilize the Census's American Community Survey (ACS) data to see how it all plays out in practice. Demographic statistics are common covariates utilized in linear regression, but they run into issues with spatial autocorrelation, which makes ACS data a great candidate for spatial regression techniques.

First, we define a set of counties we want to analyze, and return a set of variables, using the *tidycensus* package. We load in the sf package as well so we can do a spatial join later.

```{r}

library(units)
library(sf)
library(tidycensus)

dfw_counties <- c("Collin County", "Dallas", "Denton", 
                  "Ellis", "Hunt", "Kaufman", "Rockwall", 
                  "Johnson", "Parker", "Tarrant", "Wise")

variables_to_get <- c(
  median_value = "B25077_001",
  median_rooms = "B25018_001",
  median_income = "DP03_0062",
  total_population = "B01003_001",
  median_age = "B01002_001",
  pct_college = "DP02_0068P",
  pct_foreign_born = "DP02_0094P",
  pct_white = "DP05_0077P",
  median_year_built = "B25037_001",
  percent_ooh = "DP04_0046P"
)

dfw_data <- get_acs(
  geography = "tract",
  variables = variables_to_get,
  state = "TX",
  county = dfw_counties,
  geometry = TRUE,
  output = "wide",
  year = 2020
) %>%
  select(-NAME) %>%
  st_transform(32138) # NAD83 / Texas North Central

```

Next, we do some feature engineering by adding population density which represents the number of people in each Census tract per square kilo and median_structure_age, which represents median age of housing structures in tract.

```{r}

dfw_data_for_model <- dfw_data %>%
  mutate(pop_density = as.numeric(set_units(total_populationE / st_area(.), "1/km2")),
         median_structure_age = 2018 - median_year_builtE) %>%
  select(!ends_with("M")) %>% 
  rename_with(.fn = ~str_remove(.x, "E$")) %>%
  na.omit()
```

Like indicated before, the main idea behind spatial autocorrelation is that the independence of residuals are violated because the model's performance depends on geographic location.

One technique we discussed before for testing spatial autocorrelation was Moran's *I.* Let's use it

```{r}

library(spdep)

wts <- dfw_data_for_model %>%
  poly2nb() %>%
  nb2listw()

moran.test(dfw_data_for_model$residuals)

dfw_data_for_model$lagged_residuals <- lag.listw(wts, dfw_data_for_model$residuals)

ggplot(dfw_data_for_model, aes(x = residuals, y = lagged_residuals)) + 
  theme_minimal() + 
  geom_point(alpha = 0.5) + 
  geom_smooth(method = "lm", color = "red")
```

If we observe that there is spatial autocorrelation, we can use a spatial error model in order to account for this, where we capture latent spatial processes not accounted for in the model's estimation being captured in the residuals. This can be done with the errorsarlm() function in spatialreg.

```{r}

error_model <- errorsarlm(
  formula = formula2, 
  data = dfw_data_for_model, 
  listw = wts
)

summary(error_model, Nagelkerke = TRUE)
```

We look at lambda to confirm if it is large and statistically significant, which confirms to us the importance of accounting for spatial autocorrelation.
