---
title: "Final Project: Geospatial Regressions"
subtitle: "Data Science for Public Policy"
authors: "Akbar Naqvi, Thomas Tiberghien Alvarez"
format: 
  html:
    code-line-numbers: true
editor: visual
execute: 
  warning: false
self-contained: true
urlcolor: blue
---

## Background and Motivation

Geographic data is everywhere. This type of data also explains many social processes and individual decisions. As this kind of data becomes more ubiquitous, it is important to understand where and how to use it, and how to extract meaningful insights from geographical properties

Regressions are considered the bread and butter of data analysis. They have been long used in economics and social science to establish causal relationships, but data science has utilized regression methods as a backbone for prediction. Spatial regressions take the power of regressions and add an element of spatial dependency.

Space can impact how one generates data and be a form of systematic error in our analysis. Spatial analysis in its modern form has been implemented and iterated upon through many subjects: biology through plant distributions and animal movement, epidemiology through disease mapping, and most notably economics through spatial econometrics, Spatial structure helps regression model in two ways

1.  Process of generating data is spatial e.g. price of homes often dependent on area

2.  Geography can allow you to assess structure of errors and mispredictions if they are systematic, often times errors are assumed to be independently and identically distributed, but this assumption falls apart with spatial data

Spatial regression is also easier to visualize than just rows and columns of data. This makes it easier to explain insights more broadly, rather than trying to tell a story through a few data points.Â 

## Methods

### Spatial Autocorrelation

The first important question to ask when doing regression is whether it is appropriate to include a spatial element. This might seem obvious in some cases, but not necessarily all the time. A series of tests have thus been developed to test for what is called spatial autocorrelation. Spatial autocorrelation is a measure of how similar the values of a variable are within a certain geographic area. If spatial autocorrelation is not accounted for, regression models might over or underestimate relationships between dependent and independent variables and undermine the validity of the models. The most common spatial autocorrelation tests are:

-   Moran's I test

-   Geary's C test

-   Getis-Ord G test

#### Moran's I test

This test computes an index based on the distance between points. An index of -1 means that the data is evenly spread out, an index of 0 means that the data is random, and an index of 1 means that the data is clumped together. This index is then compared to critical values to see if there is significant spatial autocorrelation (based on a null hypothesis that there isn't).

Since this is the most common autocorrelation test, this is the one we will be using.

#### Geary's C test

This test is very similar to Moran's I test but is more sensitive to local spatial autocorrelation. In other words, whereas Moran's tests the global spatial autocorrelation, Geary's measures the pattern of spatial correlation within sub-regions of the data. The results are also the different to Moran's, with values closer to 0 representing positive spatial autocorrelation and higher than 1 representing negative spatial autocorrelation.

#### Getis-Ord G test

The Getis-Ord G test is a statistical test used to identify spatial patterns, such as hot spots and cold spots, in data. This test is based on a measure called the G statistic, which ranges from -infinity to +infinity. A positive G statistic indicates a hot spot, where the values of the dependent variable are higher than expected given their spatial location. A negative G statistic indicates a cold spot, where the values of the dependent variable are lower than expected given their spatial location. As before, the G statistic can then be compared to a critical value to determine whether the spatial patterns in the data are statistically significant.

### Spatial Weighting

Once you determine that there is spatial autocorrelation, it then becomes important to account for the effects of spatial proximity on the relationship between the dependent and independent variables. In geospatial regression, spatial weighting involves assigning a weight to each data point based on its distance from other data points. This weight is used to adjust the regression model, allowing it to take into account the fact that the values of the dependent variable are likely to be more similar to the values of other nearby data points than to data points that are farther away. There are also several ways to assign weights to data points:

-   Inverse distance weighting: Taking the inverse of the distance between a data point and all other data points. This method is based on the assumption that the values of the dependent variable are more likely to be similar to the values of nearby data points than to data points that are farther away. This metric is sensitive to the choice of the distance metric (meters vs kilometers), and produce unrealistic spatial weights if the data contains outliers or clusters.

-   K-nearest neighbor weighting: This method assigns a weight to each data point based on the number of other data points (k) that are within a certain distance of the data point. This method can produce unrealistic spatial weights if the data contains clusters of different sizes.

-   Rook contiguity weighting: This method assigns a weight based on whether it is contiguous (i.e., touching) with other data points. A data point is given a weight of 1 if it is contiguous with at least one other data point, and a weight of 0 if it is not. This method is easy to understand, but it only gives a binary measure and does not take into account the actual distance between points.

### Spatial Lag

### GWR/Kernel Bandwidth

------------------------------------------------------------------------

Source: Geographic Data Science in Python

**Spatial Weights**

Spatial questions target specific information about the spatial configuration of an outcome variable. When it comes to spatial analysis, we are often building a topography, a mathematical structure that expresses connectivity between observations. This weighting can help us create a *geographically weighted regression*; local version of spatial regression that generates parameters dis aggregated by spatial units of analysis , partial weights are one way to express this topology. Examples of spatial weights include

-   Contiguity Weights - A contiguous pair of spatial objects who share a common border,

-   Distance based weights - Proximity based, often observing the nearest neighbors, requires defining k nearest neighbors with spatial information used to calculate centroids

-   Block Weights - We weigh spatial elements based on a specific geographic group they belong to, and any elements not in the group disconnected from those observations, e.g. connecting counties to states

**Bringing Space Into a Regression**

Spatial regression involves introducing geographic space or context into our regressions when we feel that it plays a role or can act as a proxy for other unobserved variables. There are a variety of methods one can use to do this.

-   Spatial Feature Engineering

    -   In data science, feature engineering involves applying domain knowledge to raw data in order to structure it in a way that is meaningful, in other words, transforming a dataset to prepare it for analysis

    -   Geography is one of the best ways to introduce domiain knowledge into a data science problem.

    -   Spatial feature engineering is the process of developing additional information from raw data using geographical knowledge

    -   Simplest example -

------------------------------------------------------------------------

## Geospatial Regression with R

There are several R packages that can be used to perform geospatial regression. Some of the most popular and widely used packages include:

-   **`spdep`**: This package provides a wide range of functions for spatial data analysis, including spatial regression. It includes functions for generating spatial weights matrices, fitting spatial regression models, and evaluating the results of spatial regression analysis.

-   **`spatialreg`**: This package provides specialized functions for spatial regression analysis, including global and local spatial regression models, spatial autocorrelation analysis, and spatial error models. It also includes tools for visualizing the results of spatial regression analysis.

-   **`GWmodel`**: This package provides functions for fitting GWR models to spatial data, as well as tools for evaluating the model fit and interpreting the results. It includes functions for estimating the spatial variation in the model coefficients, as well as functions for visualizing the results of GWR analysis.

Other useful R packages when working with spatial data:

-   **`sf`**: This package provides classes and functions for working with simple features, which are a standard data format for storing spatial data in R. It includes classes for representing simple features, such as points, lines, and polygons, as well as functions for reading, writing, and manipulating simple features.

-   **`sp`**: This package provides classes and functions for working with spatial data in R. It includes classes for spatial data objects, such as points, lines, and polygons, as well as functions for reading, writing, and manipulating spatial data.

-   **`tidycensus`**: This package provides tools for working with data from the US Census Bureau. It allows users to easily download and work with Census data in R, using a "tidy" data format that is compatible with other tidyverse packages.

-   **`geosphere`**: This package provides functions for calculating distances and bearings between geographic coordinates, which can be used in spatial regression analysis. It also includes functions for generating spatial weights matrices based on distance, contiguity, or network relationships.

-   **`raster`**: This package provides functions for working with raster data, which is commonly used in geospatial analysis. It includes functions for reading, writing, and manipulating raster data, as well as functions for spatial interpolation and spatial regression.

## Application

Source: https://walker-data.com/census-r/modeling-us-census-data.html

Now that we've discussed the background and various methods used in spatial regressions, we can utilize the Census's American Community Survey (ACS) data to see how it all plays out in practice. Demographic statistics are common covariates utilized in linear regression, but they run into issues with spatial autocorrelation, which makes ACS data a great candidate for spatial regression techniques.

Using the [*tidycensus*](https://walker-data.com/census-r/modeling-us-census-data.html) package, we are interested in the effects of several predictor variables on the unemployment rate in Texas, namely the effect of the proportion of Spanish speakers in a given Census tract given Texas's high Hispanic population. We would expect that being able to speak Spanish has a negative effect on the unemployment rate in Texas, but we are interested to see how this effect varies in different regions in the state, in particular, South Texas near the Mexican border where virtually all residents speak Spanish.

We also include other demographic variables like population density, median age, percent of people with a bachelor's degree, percent of people foreign born and percent of people who are white. These predictors are important so we can control for the many different factors associated with speaking Spanish, especially in a state where Hispanic Americans are less affluent than other groups due to reasons unrelated to the language they speak. We must do some extra calculations for unemployment rate by dividing total employment by the total labor force, and also the share of Spanish speakers who do not speak English very well, dividing the number of them for a given tract by the total population.

```{r}

#load packages
library(tidyverse)
library(units)
library(sf)
library(tidycensus)


#define census variables of interest
variables<- c(
  lf = "B23025_003",
  unemployed = "B27011_008",
  speak_sp= "B06007_003",
  median_income = "DP03_0062",
  total_population = "B01003_001",
  median_age = "B01002_001",
  pct_college = "DP02_0068P",
  pct_foreign_born = "DP02_0094P",
  pct_white = "DP05_0077P"
)

#pull census data in geometric form using tidycensus

tx_data <- get_acs(
  geography = "tract",
  variables = variables,
  state = "TX",
  geometry = TRUE,
  output = "wide",
  year = 2020
) %>%
  select(-NAME) %>%
  st_transform(32138)


#calculate unemployment rate and spanish speaking rate
tx_data<- tx_data %>%
  mutate(pop_density = as.numeric(set_units(total_populationE / st_area(.), "1/km2")), unemp_rt= 100*unemployedE/lfE, speak_sp_rt = 100*speak_spE/total_populationE) %>%
  select(!ends_with("M")) %>%
  rename_with(.fn = ~str_remove(.x, "E$")) %>%
  na.omit()
```

Now that we have our data, we first do some basic exploratory data analysis by mapping our outcome variable and main predictor of interest.

```{r}

#mapping both unemployment rate and spanish speaking rate by tract
tx_unemp<- ggplot(tx_data, aes(fill = unemp_rt)) +
  geom_sf(color = NA) +
  scale_fill_viridis_c(option = "magma", direction = -1) +
  theme_void() +
  labs(fill = "Unemployment Rate (%)")
tx_sp <- ggplot(tx_data, aes(fill = speak_sp_rt)) +
  geom_sf(color = NA) +
  scale_fill_viridis_c(option = "magma", direction = -1) +
  theme_void() +
  labs(fill = "Share of people who speak Spanish (%)")

tx_unemp
tx_sp
```

We see that, as expected, there is a clear spatial variation in the share the population who speak Spanish, specifically with the region in South Texas around the Rio Grande Valley having a higher concentration. The spatial pattern in unemployment seems less clear, however we do see a slight but noticeably higher unemployment in the Rio Grande Valley area. However, this likely may be due to the fact that there is higher poverty in that area, and that a more robust analysis is needed that filters this out to look at the effect of Spanish speaking at a spatial level.

In order to establish a baseline to check for spatial autocorrelation, we want to start off by running a basic multivariate regression including our predictors and outcome variable. We do this using the "lm" package, which specializes in linear regression.

```{r}

#define linear regression model
formula <- "unemp_rt ~ speak_sp_rt + median_income + pct_college + pct_foreign_born + pct_white + median_age + pop_density + total_population"

#run and summarize model
lm_model <- lm(formula = formula, data = tx_data)

summary(lm_model)
```

We observe a negative and statistically significant partial effect of Spanish speaking on the unemployment rate . This is an outcome that aligns with our initial hypothesis. However, it is worth noting that this is an average across all tracts in Texas, and as mentioned before, South Texas has a much higher share of Spanish speakers than other parts of Texas, so we suspect that this effect will be different there than in other regions. This is why we need to check our regression for any spatial autocorrelation, and then remedy it if it exists.

Like indicated before, the main idea behind spatial autocorrelation is that the independence of residuals are violated because the model's performance depends on geographic location.

One technique we discussed before for testing spatial autocorrelation was Moran's *I.* We can use the [spdep](https://r-spatial.github.io/spdep) package to utilize this technique. This package contains a collection of functions to create spatial weights matrix objects from polygon contiguities as well as perform spatial autocorrelation tests. We use the "poly2nb" function to create a neighbors matrix and from that list, generate spatial weights. Then, we can conduct our test.

```{r}

library(spdep)

#add residuals to Texas dataframe and establish spatial weights
tx_data$residuals <- residuals(lm_model)
wts <- tx_data %>%
  poly2nb() %>%
  nb2listw()
#run spatial autocorrelation test
moran.test(tx_data$residuals, wts)

```

From the results of the rest, we can see that the Moran's I statistic is positive at just under 0.11, and statistically significant with p-value close to zero. Thus, we can conlude there is spatial autocorrelation within the linear model's residuals.

There are many ways to deal with spatial autocorrelation through regression. One may utilize a global technique like [spatial lag models or spatial error models](https://walker-data.com/census-r/modeling-us-census-data.html#spatial-regression). In both cases, the model accounts for spatial spillover effects. The spatial lag model includes a spatial lag, or the average of neighboring values of a location, of the outcome variable in the model itself, while a spatial error model includes the spatial lag within the error term. These models help capture the average effect of our predictors on our outcome variable filtering out the spatial effects which may violate our assumption of independent and identically distributed random variables.

However, since Texas is such a diverse state, we are more interested in how the effect of Spanish speaking on unemployment is regionally distributed. To see this we want to ideally be able to run a separate regression for every collection of similar and contiguous tracts in Texas that incorporates spillover effects from surrounding neighborhoods and shows us the geospatial heterogeneity of our model.

We can do this with a [Geographically Weighted Regression](https://walker-data.com/census-r/modeling-us-census-data.html#geographically-weighted-regression) (GWR). A GWR is really a set of local regressions designed to give us a coefficient for every unit of geography we are interested in. In order to do this, we can use the [GWmodel](https://www.jstatsoft.org/article/view/v063i17) package. This package includes functions for summary statistics, principal components analysis regression and discriminant analysis all in a geographic weighted framework.

First, we need to choose a kernel bandwidth, which will give us a way to calculate the nearest neighbors of each location in order to be able to incorporate neighborhood effects. We can calculate an ideal kernel bandwidth by utilizing cross validation with the function bw.gwr after turning our data into a spatial data frame.

```{r}

library(GWmodel)
library(sf)

#turn Texas dataframe into aa spatial dataframe
tx_data_sp <- tx_data %>%
as_Spatial()

#run cross-validation to find best kernel bandwidth
bw <- bw.gwr(
formula = formula,
data = tx_data_sp,
kernel = "bisquare",
adaptive = TRUE)


```

Now, we can run the regression using the bandwidth generated. Our output will be an object with both the results of a global model and the ranges of locally varying estimates. We can extract mappable model results as well using the "SDF" element, which contains a spatial polygons data frame which contains our coefficients, r-squared and other parameters for each neighborhood.

```{r}

#run GWR model using formula used for linear regression, store results in spatial data frame
gw_model <- gwr.basic(
  formula = formula, 
  data = tx_data_sp, 
  bw = bw,
  kernel = "bisquare",
  adaptive = TRUE
)

gw_model_results <- gw_model$SDF %>%
  st_as_sf() 

```

Now, we can map the partial effect of the share of people in a tract of Spanish speaking on unemployment in a given neighborhood, and look at locally varying results.

```{r}

#Now map betas for Spanish speaking on unemployment rate
ggplot(gw_model_results, aes(fill = speak_sp_rt)) + 
  geom_sf(color = NA) + 
  scale_fill_viridis_c(option = "magma") + 
  theme_void() + 
  labs(fill = "Local regression coefficients for 
       \nshare of Spanish speakers")

```

From our graph, we observe some interesting patterns. Our original regression demonstrated that being able to speak Spanish is associated with lower levels of unemployment, ceteris parabis in the state of Texas. Mapping the regional variation geospatially shows us that most of this effect is being driven by Central Texas, where the effect of the share of Spanish speakers on lowering unemployment is much stronger than any other region of the country. Also of note is that in South Texas, near the Mexican border, the share of Spanish speakers, which would be estimated to be very high, is actually associated with higher unemployment.

These results have some potential explanations. The area in Central Texas in which the coefficient demonstrates a stronger relationship is the region that surrounds the Austin metro area, which is considered a particularly fast growing and cosmopolitan region of the state. It is likely that more job opportunities exist in this region in the first place, so knowing Spanish opens one up to more opportunities in the job market. Additionally, [Central Texas is only 23% Hispanic, compared to the state as a whole which is 38% Hispanic](https://comptroller.texas.gov/economy/economic-data/regions/2020/central.php). Thus, there is more of a comparative advantage knowing Spanish there than in other parts of the state.

In South Texas, we actually see a small but positive relationship between Spanish speaking and unemployment. Considering that this area borders Mexico, it's likely that Spanish is the first language for many residents and businesses that operate there. Thus, it's possible that the people who do not speak Spanish are unique in ways that allow them to be more attractive in the job market, subsequently making Spanish-speaking positively associated with unemployment, although this is, again, a small effect.


## Conclusion and Discussion

Spatial regressions are one of several potential methods in a rising field of spatial analysis. Whether it is to deal with violations of the i.i.d assumption in order to create a more robust global model, or utilize the power of spatial mapping techniques to illustrate local relationships, it is a flexible and broad analytical tool that can be applied in many disciplines including economics, bio statistics, criminology, and others.

What we have demonstrated in this paper just scratches the surface. The future of spatial regression techniques has numerous possibilities. The recent utilization of satellite imagery, for examples, offers the opportunity to work with raw data spanning millions of observations of pixels. Such techniques are [especially important for tracking how communities are able to achieve target social and environmental goals](https://www.oecd.org/iaos2018/programme/IAOS-OECD2018_Holloway-Mengersen-Helmstedt.pdf). Some satellite imagery can even provide information on night-time imaging, which has even been [utilized in linear regression to measure housing vacancy within census tracts](https://www.mdpi.com/2072-4292/10/12/1920/htm). Additionally, more advanced AI capabilities and machine learning techniques can supplement spatial regression analysis. For example,[one analysis](https://www.tandfonline.com/doi/abs/10.1080/00045608.2012.707587) used a combination of clustering using a SKATER algorithm commonly used to determine spatial relationships between neighboring areas, and dimension reduction with Principal Component Analysis (PCA) to derive a mixed GWR\]

Overall, this emergence and power makes spatial regression analysis the future of both predictive and causal quantitative analysis in academic and policy circles. Numerous science and social science questions include space as a factor, and we have only recently found a way to use spatial regression techniques to incorporate the complexity they deserve. Thus, we believe that spatial regressions will soon become the most common form of regression analysis used to solve important policy problems., and it was prudent to examine its basics in this paper.

